\section{Results}
\label{sec:results}

This section presents the evaluation methodology, quantitative results, and qualitative analysis of the FIB agent's performance.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{evaluation-pipeline.png}
    \caption{Evaluation pipeline: questions are fed to the agent, responses are collected with tool call trajectories, and the LLM judge scores each response across seven metrics.}
    \label{fig:evaluation-pipeline}
\end{figure}

\subsection{Experimental Setup}

\subsubsection{Models Tested}
We evaluated three Gemini models as the agent backbone. Gemini 3 Flash (preview) represents the latest generation with improved instruction following. Gemini 2.5 Flash was tested as a fast, cost-effective model optimized for speed, while Gemini 2.5 Pro was evaluated for its higher capacity for complex reasoning. All Gemini models were accessed through Google Vertex AI using application default credentials. Additionally, we evaluated two local open-source models - Qwen 2.5 7B and Llama 3.2 3B - to explore privacy-preserving deployment options.

\subsubsection{Evaluation Judge}
For the LLM-as-judge evaluation, we employed Gemini 2.5 Flash Lite as the evaluator model. This lighter model provided sufficient capability for rubric-based scoring while maintaining cost efficiency across the substantial evaluation set, which involved 315 evaluations per run (45 questions multiplied by 7 metrics).

\subsubsection{Evaluation Dataset}
The dataset comprises 45 manually curated questions distributed across 10 categories, as detailed in \tabref{tab:categories}.

\begin{table}[H]
    \centering
    \caption{Question distribution by category}
    \label{tab:categories}
    \begin{tabular}{lrl}
        \toprule
        \textbf{Category} & \textbf{Count} & \textbf{Description} \\
        \midrule
        courses & 13 & Course information queries \\
        exams & 5 & Exam schedules and rooms \\
        professors & 4 & Faculty information \\
        news & 3 & FIB announcements \\
        academic & 2 & Terms and semesters \\
        classrooms & 2 & Room information \\
        personal & 4 & User-specific (requires OAuth) \\
        multi\_tool & 5 & Complex multi-aspect queries \\
        ambiguous & 4 & Vague queries needing clarification \\
        web\_search & 3 & External information queries \\
        \bottomrule
    \end{tabular}
\end{table}

The questions span four complexity levels. Simple questions are direct factual queries requiring a single tool. Multi-step questions require multiple tool calls or comparisons. Contextual questions depend on user context or implicit information. Finally, ambiguous questions are vague or incomplete queries that require the agent to ask for clarification.

\subsubsection{Evaluation Metrics}
We defined seven evaluation metrics, each accompanied by a detailed persona, rubric describing correct and incorrect behaviors, evaluation steps, and reminders for the LLM judge. Relevance measures whether the response addresses the user's question. Helpfulness assesses if the response is actionable and useful. Conciseness checks if the response is appropriately brief without losing information. Structure evaluates the organization and formatting of the response. Tone ensures the response is professional and appropriate for an academic context. Error Handling looks for graceful management of missing data or failures. Finally, Tool Appropriateness determines if the agent selected the right tools for the task. Each metric produces a score from 0.0 to 1.0 along with a textual reasoning.

\subsection{Quantitative Results}

\tabref{tab:results-comparison} presents the average scores across all 45 questions for all evaluated models, comparing proprietary Gemini models against open-source alternatives.

\begin{table}[H]
    \centering
    \caption{Evaluation results comparing proprietary and open-source models. Best scores per metric in bold.}
    \label{tab:results-comparison}
    \small
    \begin{tabular}{lcccccc}
        \toprule
        \textbf{Metric} & \textbf{Target} & \textbf{G3 Flash} & \textbf{G2.5 Flash} & \textbf{G2.5 Pro} & \textbf{Qwen 7B} & \textbf{Llama 3B} \\
        \midrule
        Relevance & $>$0.85 & \textbf{0.98} & 0.90 & 0.89 & 0.74 & 0.22 \\
        Helpfulness & $>$0.85 & \textbf{0.99} & 0.89 & 0.93 & 0.80 & 0.32 \\
        Conciseness & $>$0.80 & \textbf{0.87} & 0.85 & 0.83 & 0.69 & 0.28 \\
        Structure & $>$0.75 & 0.79 & 0.77 & \textbf{0.80} & 0.76 & 0.29 \\
        Tone & $>$0.80 & \textbf{0.89} & 0.85 & 0.85 & 0.81 & 0.37 \\
        Error Handling & $>$0.70 & \textbf{0.74} & 0.51 & 0.43 & 0.29 & 0.08 \\
        Tool Approp. & $>$0.80 & 0.75 & 0.77 & \textbf{0.78} & 0.43 & 0.09 \\
        \midrule
        \textbf{Average} & --- & \textbf{0.86} & 0.79 & 0.79 & 0.65 & 0.24 \\
        \bottomrule
    \end{tabular}
\end{table}

Several key observations emerge from these results. Gemini 3 Flash achieves the best overall performance (0.86 average) and is the only model to pass the error handling threshold (0.74 vs.\ the 0.70 target). The Gemini 2.5 models achieve identical average scores (0.79), with both meeting targets for relevance, helpfulness, conciseness, structure, and tone. Error handling remains challenging for the 2.5 generation, which analysis suggests is partly due to overly strict rubric interpretation combined with verbose error messages. Tool appropriateness is slightly below target across all Gemini models, affected by the indirect tool calls inherent in subagent delegation.

A clear capability gap exists between proprietary and open-source models. Qwen 2.5 7B achieves a reasonable 0.65 average, demonstrating that mid-sized local models can handle straightforward queries, though tool appropriateness (0.43) reveals difficulties with multi-step tool orchestration. Llama 3.2 3B performs poorly across all metrics (0.24 average), indicating that reliable agentic behavior requires models above a certain capability threshold.

\subsubsection{Performance by Category}

\tabref{tab:category-performance} shows relevance scores broken down by question category.

\begin{table}[H]
    \centering
    \caption{Relevance scores by question category (Gemini 3 Flash)}
    \label{tab:category-performance}
    \begin{tabular}{lr}
        \toprule
        \textbf{Category} & \textbf{Avg. Relevance} \\
        \midrule
        courses & 1.00 \\
        exams & 1.00 \\
        professors & 1.00 \\
        classrooms & 1.00 \\
        academic & 1.00 \\
        news & 1.00 \\
        personal & 1.00 \\
        multi\_tool & 0.98 \\
        ambiguous & 0.77 \\
        web\_search & 1.00 \\
        \bottomrule
    \end{tabular}
\end{table}

Gemini 3 Flash achieves perfect relevance scores (1.00) across eight categories, demonstrating robust handling of factual queries about courses, exams, professors, classrooms, academic terms, news, personal data, and web searches. Multi-tool queries score nearly perfect (0.98). Notably, ambiguous queries show the lowest performance (0.77), indicating that the model tends to attempt direct answers rather than asking for clarification - behavior that the rubric penalizes for vague queries where clarification would be more appropriate.

\subsection{Qualitative Analysis}

\subsubsection{Successful Patterns}

The agent demonstrates strong performance in handling simple course queries. For example, when asked "How many credits is the IA course?", the agent correctly identified the course and responded concisely with "The IA course (Artificial Intelligence) has 6 credits." This response achieved perfect scores for relevance, helpfulness, and conciseness.

The agent also handles ambiguous queries effectively. When presented with a vague request like "Help me", the agent responded by offering a helpful menu of capabilities, asking the user to specify their need regarding course information, exam schedules, professor contacts, or personal data. This approach of guiding the user rather than guessing resulted in high relevance and helpfulness scores.

\subsubsection{Challenging Cases}

Multi-tool complexity presents a greater challenge. In response to "Tell me everything about the EDA course", the agent provided a comprehensive answer including course details, professors, and exam schedules. However, the structure score suffered due to inconsistent formatting, and tool appropriateness was marked down because the agent made multiple search calls that could have been consolidated.

Error handling verbosity also proved to be an issue. When unable to find a course code, the agent tended to offer apologetic and speculative explanations rather than a direct statement. For instance, responding to a query about a non-existent course "XYZ" with a lengthy apology and potential reasons for the failure resulted in a low error handling score, as the rubric prefers more direct communication of the error.

\subsection{Discussion}

\subsubsection{Error Handling Metric Analysis}
The error handling metric significantly underperforms expectations. Upon analysis, we identified two contributing factors. First, there were false negatives where responses that correctly answered questions received low error handling scores because the judge interpreted "error handling" as requiring explicit acknowledgment of limitations even when none were encountered. Second, when errors did occur, the agent tended toward verbose apologies with speculative explanations, which the rubric penalizes. This suggests that both prompt refinement to reduce verbosity on errors and rubric clarification are needed.

\subsubsection{Tool Appropriateness and Subagent Delegation}
Tool appropriateness scores are affected by the hierarchical architecture. When the root agent delegates to the subagent via the task tool, the evaluation only sees "task" as the tool call, not the underlying tools called by the subagent. This is a limitation of the current evaluation framework that could be addressed by flattening tool call trajectories across agent boundaries or by scoring delegation as appropriate when the subagent handles the query correctly.

\subsubsection{Model Comparison}
Gemini 3 Flash demonstrates significant improvements over the 2.5 generation, particularly in error handling (0.74 vs.\ 0.51) and core metrics like relevance (0.98) and helpfulness (0.99). This suggests that newer model generations benefit substantially from improved instruction following and tool use capabilities.

Within the 2.5 generation, Flash and Pro achieved nearly identical overall scores (0.79). The Pro model showed marginal advantages in helpfulness and structure, providing slightly more comprehensive and better-formatted responses. Conversely, Flash showed advantages in conciseness, offering less verbose responses. Given the minimal quality difference and Flash's faster response times and lower cost, Gemini 2.5 Flash remains a cost-effective choice, though Gemini 3 Flash is recommended for production deployments where quality is paramount.

\subsection{Local Model Experiments}

To explore deployment options without cloud dependencies, we evaluated two local models using llama-cpp-python: Qwen 2.5 7B Instruct and Llama 3.2 3B Instruct, both in Q4\_K\_M quantization. This required implementing a custom \texttt{ChatLlamaCppTools} wrapper that formats tool definitions using Qwen's XML-based tool calling syntax, as the standard LangChain integration does not reliably generate tool calls with local models.

\subsubsection{Local Model Results}

\tabref{tab:local-model-results} compares the local models against the cloud-based Gemini models on key metrics. We tested each local model at two temperature settings (0.1 and 0.5) to assess the impact of sampling randomness on tool-calling reliability.

\begin{table}[H]
    \centering
    \caption{Evaluation results comparing cloud and local models}
    \label{tab:local-model-results}
    \begin{tabular}{llrrr}
        \toprule
        \textbf{Model} & \textbf{Temp} & \textbf{Relevance} & \textbf{Helpfulness} & \textbf{Tool Approp.} \\
        \midrule
        Gemini 3 Flash & -- & 0.98 & 0.99 & 0.75 \\
        Gemini 2.5 Flash & -- & 0.90 & 0.89 & 0.77 \\
        Gemini 2.5 Pro & -- & 0.89 & 0.93 & 0.78 \\
        \midrule
        Qwen 2.5 7B & 0.1 & 0.74 & 0.80 & 0.42 \\
        Qwen 2.5 7B & 0.5 & 0.74 & 0.80 & 0.43 \\
        \midrule
        Llama 3.2 3B & 0.1 & 0.22 & 0.29 & 0.10 \\
        Llama 3.2 3B & 0.5 & 0.22 & 0.32 & 0.09 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Analysis}

The results reveal a significant capability gap between model sizes for agentic tasks. Qwen 2.5 7B achieves approximately 75--80\% of Gemini 2.5's relevance and helpfulness scores, demonstrating that mid-sized local models can handle straightforward queries. However, its tool appropriateness score (0.42) is notably lower than the Gemini models (0.75--0.78), indicating difficulties with complex multi-step tool orchestration. Gemini 3 Flash sets a new benchmark with near-perfect relevance (0.98) and helpfulness (0.99) scores.

Llama 3.2 3B performs poorly across all metrics, with relevance below 0.25. Analysis of the inference logs reveals that the 3B model frequently fails to generate properly formatted tool calls, often responding with generic text instead of invoking the appropriate API tools. This suggests that reliable agentic behavior requires models above a certain capability threshold.

Interestingly, temperature has negligible impact on performance for either local model. This contrasts with common intuition that lower temperatures improve tool-calling reliability, and suggests that the primary bottleneck is model capability rather than sampling strategy.

These findings support using Gemini 3 Flash for production deployments where quality is paramount, while demonstrating that local models like Qwen 2.5 7B could serve as cost-effective alternatives for simpler use cases or privacy-sensitive environments. The modular model strategy pattern in our implementation enables easy swapping between cloud and local backends.
